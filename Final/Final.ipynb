{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.elasticsearch:elasticsearch-hadoop:7.4.2 pyspark-shell'\n",
    "\n",
    "# IP = 'da2019w-1019.eastus.cloudapp.azure.com'\n",
    "IP = '10.0.0.25'\n",
    "\n",
    "HERE_API_KEY = 'TarqgkWPPRHbWkLVZWCz2VAGJVHWj_B18ii-yO5pyZo'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"GMT\")\n",
    "\n",
    "es = Elasticsearch([{'host': IP}])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_elastic(index, query=\"\", scroll_size=\"10000\", array_field=\"\"):\n",
    "    if not es.indices.exists(index):\n",
    "        raise Exception(\"Index doesn't exist!\")\n",
    "\n",
    "    return spark.read\\\n",
    "                .format(\"org.elasticsearch.spark.sql\")\\\n",
    "                .option(\"es.nodes.wan.only\",\"true\")\\\n",
    "                .option(\"es.port\",\"9200\")\\\n",
    "                .option(\"es.nodes\",IP)\\\n",
    "                .option(\"es.nodes.client.only\", \"false\")\\\n",
    "                .option(\"pushdown\", \"true\")\\\n",
    "                .option(\"es.query\", query)\\\n",
    "                .option(\"es.scroll.size\", scroll_size)\\\n",
    "                .option(\"es.scroll.keepalive\", \"120m\")\\\n",
    "                .option(\"es.read.field.as.array.include\", array_field)\\\n",
    "                .load(index)\n",
    "\n",
    "        \n",
    "DEFUALT_SCEHMA = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"actualDelay\" : { \"type\": \"long\" },\n",
    "            \"filteredActualDelay\" : { \"type\": \"long\" },\n",
    "            \"delay\" : { \"type\": \"long\" },\n",
    "            \"areaId\" : { \"type\": \"long\" },\n",
    "            \"areaId1\" : { \"type\": \"long\" },\n",
    "            \"areaId2\" : { \"type\": \"long\" },\n",
    "            \"areaId3\" : { \"type\": \"long\" },\n",
    "            \"atStop\" : { \"type\": \"boolean\" },\n",
    "            \"busStop\" : { \"type\": \"long\" },\n",
    "            \"congestion\" : { \"type\": \"boolean\" },\n",
    "            \"gridID\" : { \"type\": \"keyword\" },\n",
    "            \"journeyPatternId\" : { \"type\": \"keyword\" },\n",
    "            \"lineId\" : { \"type\": \"keyword\" },\n",
    "            \"coordinates\" : { \"type\": \"geo_point\" },\n",
    "            \"timestamp\" : { \"type\": \"date\", \"format\" : \"epoch_millis\" },\n",
    "            \"vehicleId\" : { \"type\": \"long\" },\n",
    "            \"dateTime\" : { \"type\": \"date\" }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def write_to_elastic(df, index: str, settings=DEFUALT_SCEHMA, append=True):\n",
    "    if es.indices.exists(index) and not append:\n",
    "        es.indices.delete(index=index)\n",
    "    \n",
    "    es.indices.create(index=index, ignore=400, body=settings)\n",
    "\n",
    "    df.write.format(\"org.elasticsearch.spark.sql\")\\\n",
    "        .option(\"es.resource\", index)\\\n",
    "        .option(\"es.nodes.wan.only\",\"true\")\\\n",
    "        .option(\"es.port\",\"9200\")\\\n",
    "        .option(\"es.nodes\",IP)\\\n",
    "        .option(\"es.nodes.client.only\", \"false\")\\\n",
    "        .save()\n",
    "\n",
    "@F.udf(ArrayType(DoubleType()))\n",
    "def get_station_coord(stopNumber):\n",
    "    res = requests.get(f\"https://data.smartdublin.ie/cgi-bin/rtpi/busstopinformation?stopid={stopNumber}\").json()\n",
    "    if not res['errorcode'] == '0' or not res['results']:\n",
    "        return [0, 0]\n",
    "    res = res['results'][0]\n",
    "    return [float(res['longitude']), float(res['latitude'])]\n",
    "\n",
    "\n",
    "def calculate_centroids(df):\n",
    "    centroid_df = df.groupBy('busStop')\\\n",
    "                    .agg(F.mean(df.coordinates[0]).alias('centroid_longitude'), \n",
    "                            F.mean(df.coordinates[1]).alias('centroid_latitude'))\n",
    "\n",
    "    centroid_df = centroid_df.withColumn(\"coordinates\", F.array('centroid_longitude', 'centroid_latitude'))\\\n",
    "                                .drop('centroid_longitude', 'centroid_latitude')\n",
    "    return centroid_df\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "@F.udf(\"float\")\n",
    "def get_distance(coord_a, coord_b):\n",
    "    longit_a, latit_a = coord_a\n",
    "    longit_b, latit_b = coord_b\n",
    "    if None in [longit_a, latit_a, longit_b, latit_b]:\n",
    "        return 9999\n",
    "    # Transform to radians\n",
    "    longit_a, latit_a, longit_b, latit_b = map(radians, [longit_a,  latit_a, longit_b, latit_b])\n",
    "    dist_longit = longit_b - longit_a\n",
    "    dist_latit = latit_b - latit_a\n",
    "    # Calculate area\n",
    "    area = sin(dist_latit/2)**2 + cos(latit_a) * cos(latit_b) * sin(dist_longit/2)**2\n",
    "    # Calculate the central angle\n",
    "    central_angle = 2 * asin(sqrt(area))\n",
    "    radius = 6371\n",
    "    # Calculate Distance\n",
    "    distance = central_angle * radius\n",
    "    return abs(round(distance, 4))\n",
    "\n",
    "def add_distance_to_centroid(centroid_df, stop_df, drop_centroid_col=True):\n",
    "    c_df = centroid_df.selectExpr(\"coordinates as c_coordinates\", \"busStop as c_busStop\")\n",
    "    left_join = stop_df.join(c_df, stop_df['busStop'] == c_df['c_busStop'], how='inner')\n",
    "    res = left_join.withColumn('distance', get_distance(left_join.c_coordinates, left_join.coordinates)).drop('c_busStop')\n",
    "    if drop_centroid_col:\n",
    "        return res.drop('c_coordinates')\n",
    "    return res\n",
    "\n",
    "@F.udf(ArrayType(DoubleType()))\n",
    "def merge_coordinates(longitude, latitude):\n",
    "    return [float(longitude), float(latitude)]\n",
    "\n",
    "@F.udf(\"float\")\n",
    "def normalize_text_distance(name1, name2, distance):\n",
    "    return distance / max(len(name1), len(name2))\n",
    "\n",
    "\n",
    "def lev_distance(s1,s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1\n",
    "    distances = range(len(s1) + 1)\n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1])\n",
    "            else:\n",
    "                newDistances.append(1 + min((distances[index1],\n",
    "                                             distances[index1+1],\n",
    "                                             newDistances[-1])))\n",
    "        distances = newDistances\n",
    "    return distances[-1]\n",
    "\n",
    "@F.udf('float')\n",
    "def get_text_distance(station_name, reverse_gecode):\n",
    "    lev_dist = 2**10\n",
    "    \n",
    "    if reverse_gecode and station_name:\n",
    "        lev_dist = min([lev_distance(station_name, address)/max(len(station_name), len(address)) for address in reverse_gecode if address])\n",
    "    return lev_dist\n",
    "\n",
    "\n",
    "STOPWORDS = ['avenue', 'ave', 'blvd', 'boulevard', 'box', 'cir', 'court', 'ct', 'drive', 'dr', 'lane', 'ln', 'loop', 'lp', 'pl', 'place', 'po', 'pob', 'pt', 'rd', 'road', 'route', 'rr', 'rte', 'rural', 'sq', 'st', 'ste', 'street', 'suit', 'trl', 'way', 'wy']\n",
    "\n",
    "def extract_address(result):\n",
    "    address = []\n",
    "    try:\n",
    "        address = result['Location']['Address']['Street']\n",
    "    except:\n",
    "        address = result['Location']['Address']['Label']\n",
    "    return ' '.join(filter(lambda word: word.lower().rstrip('.') not in STOPWORDS, address.split()))\n",
    "\n",
    "@F.udf(ArrayType(StringType()))\n",
    "def reverse_gecode(coords):\n",
    "    lng, lat = coords\n",
    "    params = {'prox' : f\"{lat}, {lng}, 5\", 'mode' : 'retrieveAddresses', 'apiKey' : HERE_API_KEY}\n",
    "    results = requests.get(\"https://reverse.geocoder.ls.hereapi.com/6.2/reversegeocode.json\", params=params)\\\n",
    "                                                                                    .json()['Response']['View'][0]['Result']\n",
    "    addresses = list(set(map(extract_address, results)))\n",
    "    \n",
    "    return addresses\n",
    "\n",
    "\n",
    "@F.udf(BooleanType())\n",
    "def is_approx_near(coords_a, coords_b, decimal=5):\n",
    "    lng_a, lat_a = coords_a\n",
    "    lng_b, lat_b = coords_b\n",
    "\n",
    "    return (round(lng_a, decimal) == round(lng_b, decimal)) and (round(lat_a, decimal) == round(lat_b, decimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('_id',StructType([StructField('$oid',StringType(),True)]),True),\n",
    "                     StructField('actualDelay',LongType(),True),\n",
    "                     StructField('angle',DoubleType(),True),\n",
    "                     StructField('anomaly',BooleanType(),True),\n",
    "                     StructField('areaId',LongType(),True),\n",
    "                     StructField('areaId1',LongType(),True),\n",
    "                     StructField('areaId2',LongType(),True),\n",
    "                     StructField('areaId3',LongType(),True),\n",
    "                     StructField('atStop',BooleanType(),True),\n",
    "                     StructField('busStop',LongType(),True),\n",
    "                     StructField('calendar',StructType([StructField('$numberLong',StringType(),True)]),True),\n",
    "                     StructField('congestion',BooleanType(),True),\n",
    "                     StructField('currentHour',LongType(),True),\n",
    "                     StructField('dateType',LongType(),True),\n",
    "                     StructField('dateTypeEnum',StringType(),True),\n",
    "                     StructField('delay',LongType(),True),\n",
    "                     StructField('direction',LongType(),True),\n",
    "                     StructField('distanceCovered',DoubleType(),True),\n",
    "                     StructField('ellapsedTime',LongType(),True),\n",
    "                     StructField('filteredActualDelay',LongType(),True),\n",
    "                     StructField('gridID',StringType(),True),\n",
    "                     StructField('journeyPatternId',StringType(),True),\n",
    "                     StructField('justLeftStop',BooleanType(),True),\n",
    "                     StructField('justStopped',BooleanType(),True),\n",
    "                     StructField('latitude',DoubleType(),True),\n",
    "                     StructField('lineId',StringType(),True),\n",
    "                     StructField('loc',StructType([StructField('coordinates',ArrayType(DoubleType(),True),True),StructField('type',StringType(),True)]),True),\n",
    "                     StructField('longitude',DoubleType(),True),\n",
    "                     StructField('poiId',LongType(),True),\n",
    "                     StructField('poiId2',LongType(),True),\n",
    "                     StructField('probability',DoubleType(),True),\n",
    "                     StructField('systemTimestamp',DoubleType(),True),\n",
    "                     StructField('timestamp',StructType([StructField('$numberLong',StringType(),True)]),True),\n",
    "                     StructField('vehicleId',LongType(),True),\n",
    "                     StructField('vehicleSpeed',LongType(),True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('/datashare/busFile', schema=schema)\\\n",
    "        .withColumn('timestamp', df['timestamp']['$numberLong'].cast('bigint'))\\\n",
    "        .withColumn('dateTime', F.to_timestamp((df['timestamp'] / 1000)))\\\n",
    "        .withColumn('coordinates', df['loc']['coordinates'])\\\n",
    "        .drop(  \n",
    "                '_id',\n",
    "                'loc',\n",
    "                'calendar',\n",
    "                'currentHour', \n",
    "                'systemTimestamp', \n",
    "                'dateType', \n",
    "                'dateTypeEnum',\n",
    "                'direction',\n",
    "                'poiId',\n",
    "                'poiId2',\n",
    "                'anomaly',\n",
    "                'probability',\n",
    "                'distanceCovered',\n",
    "                'ellapsedTime',\n",
    "                'vehicleSpeed',\n",
    "                'justLeftStop',\n",
    "                'justStopped',\n",
    "                'angle')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read from Stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StreamingPath = \"/datashare/busFile\"\n",
    "\n",
    "inputDf = spark.readStream\\\n",
    "                .json(StreamingPath, schema=schema)\\\n",
    "                .withColumn('timestamp', df['timestamp']['$numberLong'].cast('bigint'))\\\n",
    "                .withColumn('dateTime', F.to_timestamp((df['timestamp'] / 1000)))\\\n",
    "                .withColumn('coordinates', df['loc']['coordinates'])\\\n",
    "                .drop(  \n",
    "                        '_id',\n",
    "                        'loc',\n",
    "                        'calendar',\n",
    "                        'currentHour', \n",
    "                        'systemTimestamp', \n",
    "                        'dateType', \n",
    "                        'dateTypeEnum',\n",
    "                        'direction',\n",
    "                        'poiId',\n",
    "                        'poiId2',\n",
    "                        'anomaly',\n",
    "                        'probability',\n",
    "                        'distanceCovered',\n",
    "                        'ellapsedTime',\n",
    "                        'vehicleSpeed',\n",
    "                        'justLeftStop',\n",
    "                        'justStopped',\n",
    "                        'angle')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to DWH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_elastic(df, \"dublin-bus-full\", append=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate station centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dublin_bus_at_stop = read_elastic('dublin-bus-full').filter(\"atStop\")\n",
    "# write_to_elastic(dublin_bus_at_stop, \"dublin-bus-at-stop\", append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q =\"\"\"{\n",
    "  \"query\": {\n",
    "    \"filtered\": {\n",
    "      \"filter\": {\n",
    "        \"exists\": {\n",
    "          \"field\": \"label\"\n",
    "        }\n",
    "      },\n",
    "      \"query\": {\n",
    "        \"match_all\": {}\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dublin_bus_at_stop = read_elastic('dublin-bus-full').filter(\"atStop\")\n",
    "# TODO: Calculate centroids through Elastic query\n",
    "\n",
    "'''\n",
    "POST /dublin-bus-full/_search?size=1000\n",
    "{\n",
    "  \"query\": {\n",
    "      \"bool\": {\n",
    "        \"must\": {\n",
    "            \"term\": {\n",
    "                \"atStop\": \"true\"\n",
    "\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"aggs\" : {\n",
    "        \"busStop\" : {\n",
    "            \"terms\" : { \"field\" : \"busStop\" },\n",
    "            \"aggs\" : {\n",
    "                \"centroid\" : {\n",
    "                    \"geo_centroid\" : { \"field\" : \"coordinates\" }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "index = 'stop-index'\n",
    "\n",
    "search = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\" : {\n",
    "        \"busStop\" : {\n",
    "\n",
    "            \"terms\" : {\n",
    "                     \"field\" : \"busStop\"\n",
    "            },\n",
    "            \"aggregations\" : { \n",
    "                \"lineId\": {\n",
    "                    \"terms\": {\"field\": \"lineId\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "res = es.search(index=index, body=search)\n",
    "\n",
    "\n",
    "\n",
    "centroid_df = calculate_centroids(dublin_bus_at_stop.select(\"busStop\", \"coordinates\"))\n",
    "\n",
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"busStop\" : { \"type\": \"long\" },\n",
    "            \"coordinates\" : { \"type\": \"geo_point\" },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "write_to_elastic(centroid_df, index=\"dublin-bus-at-stop-station-centroids\", settings=settings, append=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter distance > 0.5Km :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_centroid_df = read_elastic(\"stop-centroid-index\")\n",
    "\n",
    "filter_stop = add_distance_to_centroid(es_centroid_df, es_df).filter(\"distance < 0.5\").drop('distance')\n",
    "\n",
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"actualDelay\" : { \"type\": \"long\" },\n",
    "            \"areaId\" : { \"type\": \"long\" },\n",
    "            \"areaId1\" : { \"type\": \"long\" },\n",
    "            \"areaId2\" : { \"type\": \"long\" },\n",
    "            \"areaId3\" : { \"type\": \"long\" },\n",
    "            \"atStop\" : { \"type\": \"boolean\" },\n",
    "            \"busStop\" : { \"type\": \"long\" },\n",
    "            \"congestion\" : { \"type\": \"boolean\" },\n",
    "            \"gridID\" : { \"type\": \"keyword\" },\n",
    "            \"journeyPatternId\" : { \"type\": \"keyword\" },\n",
    "            \"lineId\" : { \"type\": \"keyword\" },\n",
    "            \"coordinates\" : { \"type\": \"geo_point\" },\n",
    "            \"timestamp\" : { \"type\": \"date\", \"format\" : \"epoch_millis\" },\n",
    "            \"vehicleId\" : { \"type\": \"long\" },\n",
    "            \"dateTime\" : { \"type\": \"date\" }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "write_to_elastic(filter_stop, 'filter-500-index', settings=settings, append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cenroids(index):\n",
    "    centroids = []\n",
    "    num_partitions = 10\n",
    "\n",
    "    for partition in range(num_partitions):\n",
    "        search = {\n",
    "            \"size\" : 1,\n",
    "            \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"term\": {\n",
    "                        \"atStop\": \"true\"\n",
    "\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            },\n",
    "            \"aggs\" : {\n",
    "                \"busStop\" : {\n",
    "                    \"terms\" : { \n",
    "                    \"field\" : \"busStop\",\n",
    "                    \"size\" : 5000,\n",
    "                    \"include\": {\n",
    "                    \"partition\": partition,\n",
    "                    \"num_partitions\": num_partitions\n",
    "                        }\n",
    "                    },\n",
    "                    \"aggs\" : {\n",
    "                        \"centroid\" : {\n",
    "                            \"geo_centroid\" : { \"field\" : \"coordinates\" }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "        }\n",
    "        res = es.search(index=index, body=search)\n",
    "        for bucket1 in res['aggregations']['busStop']['buckets']:\n",
    "            busStop = bucket1['key']\n",
    "            coordinates = [bucket1['centroid']['location']['lon'],  bucket1['centroid']['location']['lat']]\n",
    "            centroids.append((busStop, coordinates))\n",
    "                \n",
    "    schema = StructType([StructField('busStop',LongType(),True),\n",
    "                        StructField('coordinates',ArrayType(DoubleType(),True))])\n",
    "    centroids_df = spark.createDataFrame(centroids, schema)\n",
    "    return centroids_df"
   ]
  }
 ]
}