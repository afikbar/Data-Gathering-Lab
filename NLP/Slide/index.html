<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><title>Neural Machine Translation</title><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport"><link href="node_modules/reveal.js/css/reveal.css" rel="stylesheet"><link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme"><!--This CSS is generated by the Asciidoctor-Reveal.js converter to further integrate AsciiDoc's existing semantic with Reveal.js--><style type="text/css">.reveal div.right {
  float: right;
}

/* callouts */
.conum[data-value] {display:inline-block;color:#fff!important;background-color:rgba(50,150,50,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}</style><link href="node_modules/reveal.js/lib/css/zenburn.css" rel="stylesheet"><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? "node_modules/reveal.js/css/print/pdf.css" : "node_modules/reveal.js/css/print/paper.css";
document.getElementsByTagName( 'head' )[0].appendChild( link );</script><!--[if lt IE 9]><script src="node_modules/reveal.js/lib/js/html5shiv.js"></script><![endif]--></head><body><div class="reveal"><div class="slides"><section class="title" data-state="title"><h1>Neural Machine Translation</h1></section><section id="_googles_neural_machine_translation"><h2>Google’s Neural Machine Translation</h2><div class="paragraph"><p>Bridging the Gap between Human and Machine Translation</p></div></section>
<section><section id="_introduction"><h2>Introduction</h2><div class="ulist"><ul><li class="fragment"><p>Machine Translation<br>
Involves rules for converting text. The rules are often developed by linguists and may operate at the lexical, syntactic, or semantic level.</p></li><li class="fragment"><p>Statistical Machine Translation<br>
Given a sentence <em>T</em>, we seek the sentence <em>S</em> from which the translator produced <em>T</em>. Thus, we wish to maximize <em>Pr(S|T)</em>.</p></li><li class="fragment"><p>Neural Machine Translation<br>
The strength of NMT lies in its ability to learn directly, in an end-to-end fashion, the mapping from input text to associated output text.</p></li></ul></div><aside class="notes"><div class="paragraph"><p>Given a sequence of text in a source language, there is no one single best translation of that text to another language. This is because of the natural ambiguity and flexibility of human language.</p></div>
<div class="ulist"><ul><li><p>Rule-based Machine Translation, or RBMT.<br>
The key limitations of the classical machine translation approaches are both the expertise required to develop the rules, and the vast number of rules and exceptions required.</p></li><li><p>The approach is data-driven, requiring only a corpus of examples with both source and target language text. This means linguists are not longer required to specify the rules of translation.</p></li><li><p>Unlike the traditional phrase-based translation system which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a correct translation.</p></li></ul></div></aside></section><section id="_quote"><div class="imageblock" style=""><img src="../images/translate-models.png" alt="translate models" width="60%"></div>
<div class="quoteblock"><blockquote><div class="paragraph"><p>If you talk to a man in a language he understands, that goes to his head.<br>
If you talk to him in his own language, that goes to his heart.</p></div></blockquote><div class="attribution">&#8212; Nelson Mandela</div></div></section></section>
<section><section id="_seq2seq_modeling"><h2>Seq2Seq Modeling</h2><div class="paragraph"><p>Since both the input and output are sentences, we can address them as a sequence of words going in and out of a model.
This is the basic idea of Sequence-to-Sequence modeling.</p></div><div class="imageblock" style=""><img src="../images/seq2seq.png" alt="seq2seq"></div><div class="title">Figure 1. Sequence-to-Sequence model</div></section><section id="_encoderdecoder"><h2>Encoder\Decoder</h2><div class="paragraph"><p>An encoder neural network reads and encodes a source sentence into a fixed-length vector.<br>
A decoder outputs a translation from the encoded vector.<br>
The whole encoder–decoder system, is jointly trained to maximize the probability of a correct translation given a source sentence.</p></div>
<div class="imageblock" style=""><img src="../images/enc-dec.png" alt="enc dec" width="70%"></div><div class="title">Figure 2. Encoder Decoder architecture</div></section><section id="_but_how"><h2>But, how?</h2><div class="paragraph"><p>Encoder and the Decoder are both recurrent neural networks (RNN).
A more complex types of RNN are usually used, such as Gated recurrent units (GRU) &amp; Long-Short-Term-Memory (LSTM).</p></div>
<div class="imageblock" style=""><img src="../images/encoder-exp.png" alt="encoder exp"></div><div class="title">Figure 3. Encoding “the cat likes to eat pizza”</div></section><section id="_long_short_term_memory"><h2>Long-Short-Term-Memory</h2><div class="paragraph"><p>GNMT uses LSTMs for encoding and decoding.<br>
The problem is, that deep (stacked) LSTMs are hard to train (vanishing\exploding gradient).</p></div></section><section id="_long_short_term_memory_2"><h2>Long-Short-Term-Memory</h2><div class="imageblock" style=""><img src="../images/res-lstm.png" alt="res lstm"></div><div class="title">Figure 4. On the left: simple stacked LSTM layer. On the right: GNMT implementation of stacked LSTM layers with residual connections.</div>
<aside class="notes"><div class="paragraph"><p>With residual connections, input to the bottom LSTM layer ( x_i^0 ’s to LSTM_1)
is element-wise added to the output from the bottom layer ( x_i^1 ’s). This sum is then fed to the top LSTM layer (LSTM_2) as the new input.</p></div></aside></section><section id="_attention"><h2>Attention</h2><div class="paragraph"><p>The following visualization shows the progression of GNMT as it translates a Chinese sentence to English</p></div>
<div class="imageblock" style=""><img src="../images/gnmt.gif" alt="gnmt"></div>
<aside class="notes"><div class="paragraph"><p>First, the network encodes the Chinese words as a list of vectors, where each vector represents the meaning of all words read so far (“Encoder”). Once the entire sentence is read, the decoder begins, generating the English sentence one word at a time (“Decoder”). To generate the translated word at each step, the decoder pays attention to a weighted distribution over the encoded Chinese vectors most relevant to generate the English word (“Attention”; the blue link transparency represents how much the decoder pays attention to an encoded word).</p></div></aside></section></section>
<section id="_model_architecture"><h2>Model Architecture</h2><div class="imageblock" style=""><img src="../images/entire-model.png" alt="entire model"></div>
<aside class="notes"><div class="paragraph"><p>On the left is the encoder network, on the right is the decoder network, in the middle is the attention module.</p></div>
<div class="paragraph"><p>The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green
nodes gather information from right to left.</p></div>
<div class="paragraph"><p>The other layers of the encoder are uni-directional.</p></div>
<div class="paragraph"><p>Residual connections start from the layer third from the bottom in the encoder and decoder.</p></div>
<div class="paragraph"><p>The model is partitioned into multiple GPUs to speed up training.</p></div>
<div class="paragraph"><p>We have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers.
With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine.</p></div>
<div class="paragraph"><p>During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers
can start computing, each on a separate GPU.</p></div>
<div class="paragraph"><p>To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers.</p></div>
<div class="paragraph"><p>The softmax layer is also partitioned and placed on multiple GPUs.</p></div>
<div class="paragraph"><p>Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.</p></div></aside></section>
<section id="_translation"><h2>Translation</h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:16.6666%"><col style="width:83.3334%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Input</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Uno no es lo que es por lo que escribe, sino por lo que ha leído.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">SMT</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">One is not what is for what he writes, but for what he has read.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">GNMT</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">You are not what you write, but what you have read.</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Human</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">You are who you are not because of what you have written, but because of what you have read.</p></td></tr></table></section>
<section id="_overall_results"><h2>Overall Results</h2><table class="tableblock frame-all grid-all" style="width:70%"><colgroup><col style="width:20%"><col style="width:20%"><col style="width:20%"><col style="width:20%"><col style="width:20%"></colgroup><thead><tr><th class="tableblock halign-left valign-top">Languages</th><th class="tableblock halign-left valign-top">SMT</th><th class="tableblock halign-left valign-top">GNMT</th><th class="tableblock halign-left valign-top">Human</th><th class="tableblock halign-left valign-top">Improvement</th></tr><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">English&#8594;Spanish</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">4.885</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.428</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.504</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">87%</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">English&#8594;French</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">4.932</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.295</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.496</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">64%</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">English&#8594;Chinese</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">4.035</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">4.594</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">4.987</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">58%</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Spanish&#8594;English</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">4.872</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.187</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.372</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">63%</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">French&#8594;English</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.046</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.343</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">5.404</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">83%</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Chinese&#8594;English</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">3.694</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">4.263</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">4.636</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">60%</p></td></tr></table>
<aside class="notes"><div class="ulist"><ul><li><p>results show that our model reduces translation errors by more than 60% compared to the PBMT model on
these major pairs of languages</p></li><li><p>human perception of the translation quality. We asked
human raters to rate translations in a three-way side-by-side comparison.</p></li></ul></div>
<div class="paragraph"><p>The three sides are from:
1) translations from the production phrase-based statistical translation system used by Google,
2) translations from our GNMT system
3) translations by humans fluent in both languages.</p></div></aside></section>
<section id="_additional_notes"><h2>Additional notes</h2><div class="paragraph"><p>Google used this beast to provide <strong>Zero-Shot translation</strong>.</p></div>
<div class="imageblock" style=""><img src="../images/zero-shot.gif" alt="zero shot"></div>
<aside class="notes"><div class="paragraph"><p>The proposed architecture requires no change in the base GNMT system, but instead uses an additional “token” at the beginning of the input sentence to specify the required target language to translate to.</p></div>
<div class="paragraph"><p>In addition to improving translation quality, our method also enables “Zero-Shot Translation” — translation between language pairs never seen explicitly by the system.</p></div>
<div class="paragraph"><p>Here’s how it works.
Let’s say we train a multilingual system with Japanese⇄English and Korean⇄English examples, shown by the solid blue lines in the animation.
Our multilingual system, with the same size as a single GNMT system, shares its parameters to translate between these four different language pairs.
This sharing enables the system to transfer the “translation knowledge” from one language pair to the others.
This transfer learning and the need to translate between multiple languages forces the system to better use its modeling power.</p></div>
<div class="paragraph"><p>This inspired us to ask the following question: Can we translate between a language pair which the system has never seen before?
An example of this would be translations between Korean and Japanese where Korean⇄Japanese examples were not shown to the system.</p></div>
<div class="paragraph"><p>Impressively, the answer is yes — it can generate reasonable Korean⇄Japanese translations, even though it has never been taught to do so.
We call this “zero-shot” translation, shown by the yellow dotted lines in the animation.
To the best of our knowledge, this is the first time this type of transfer learning has worked in Machine Translation.</p></div></aside></section>
<section id="_questions"><h2>Questions?</h2><div class="imageblock" style=""><img src="../images/que.gif" alt="que"></div></section></div></div><script src="node_modules/reveal.js/lib/js/head.min.js"></script><script src="node_modules/reveal.js/js/reveal.js"></script><script>Array.prototype.slice.call(document.querySelectorAll('.slides section')).forEach(function(slide) {
  if (slide.getAttribute('data-background-color')) return;
  // user needs to explicitly say he wants CSS color to override otherwise we might break custom css or theme (#226)
  if (!(slide.classList.contains('canvas') || slide.classList.contains('background'))) return;
  var bgColor = getComputedStyle(slide).backgroundColor;
  if (bgColor !== 'rgba(0, 0, 0, 0)' && bgColor !== 'transparent') {
    slide.setAttribute('data-background-color', bgColor);
    slide.style.backgroundColor = 'transparent';
  }
})

// See https://github.com/hakimel/reveal.js#configuration for a full list of configuration options
Reveal.initialize({
  // Display presentation control arrows
  controls: true,
  // Help the user learn the controls by providing hints, for example by
  // bouncing the down arrow when they first encounter a vertical slide
  controlsTutorial: true,
  // Determines where controls appear, "edges" or "bottom-right"
  controlsLayout: 'bottom-right',
  // Visibility rule for backwards navigation arrows; "faded", "hidden"
  // or "visible"
  controlsBackArrows: 'faded',
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: false,
  // Control which views the slide number displays on
  showSlideNumber: 'all',
  // Push each slide change to the browser history
  history: true,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags whether to include the current fragment in the URL,
  // so that reloading brings you to the same fragment position
  fragmentInURL: false,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Use this method for navigation when auto-sliding
  autoSlideMethod: Reveal.navigateNext,
  // Specify the average time in seconds that you think you will spend
  // presenting each slide. This is used to show a pacing timer in the
  // speaker view
  defaultTiming: 120,
  // Enable slide navigation via mouse wheel
  mouseWheel: true,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  // Add `data-preview-link` and `data-preview-link="false"` to customise each link
  // individually
  previewLinks: false,
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: 'default',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'default',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',
  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  // The display mode that will be used to show slides
  display: 'block',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.5,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'node_modules/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      
      { src: 'node_modules/reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'node_modules/reveal.js/plugin/notes/notes.js', async: true },
      
      
      
      
  ],

  

});</script></body></html>