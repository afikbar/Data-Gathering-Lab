include::_settings_reveal.adoc[]

// include::_settings_deck.adoc[]

= Neural Machine Translation

== Google’s Neural Machine Translation

Bridging the Gap between Human and Machine Translation


== Introduction

[%step]
* Machine Translation +
Involves rules for converting text. The rules are often developed by linguists and may operate at the lexical, syntactic, or semantic level.

* Statistical Machine Translation +
Given a sentence __T__, we seek the sentence __S__ from which the translator produced __T__. Thus, we wish to maximize __Pr(S|T)__.

[.notes]
--
Given a sequence of text in a source language, there is no one single best translation of that text to another language. This is because of the natural ambiguity and flexibility of human language.

* Rule-based Machine Translation, or RBMT. +
The key limitations of the classical machine translation approaches are both the expertise required to develop the rules, and the vast number of rules and exceptions required.

* The approach is data-driven, requiring only a corpus of examples with both source and target language text. This means linguists are not longer required to specify the rules of translation.
--


=== Neural Machine Translation

Neural machine translation, or NMT for short, is the use of neural network models to learn a statistical model for machine translation.

The strength of NMT lies in its ability to learn directly, in an end-to-end fashion, the mapping from input text to associated output text.

[.notes]
--
* Unlike the traditional phrase-based translation system which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a correct translation.

--

[%notitle]
== Quote

[quote,Nelson Mandela]
____
If you talk to a man in a language he understands, that goes to his head. +
If you talk to him in his own language, that goes to his heart.
____


== Seq2Seq Modeling

Since both the input and output are sentences, we can address them as a sequence of words going in and out of a model.
This is the basic idea of Sequence-to-Sequence modeling.

.Sequence-to-Sequence model
image::..\images\seq2seq.png[]

=== Encoder\Decoder

An encoder neural network reads and encodes a source sentence into a fixed-length vector. +
A decoder "translates" the encoded vector. 

The whole encoder–decoder system, is jointly trained to maximize the probability of a correct translation given a source sentence.

.Encoder Decoder architecture
image::..\images\enc-dec.png[,70%]

=== But, how?

The encoder and decoder are both recurrent neural networks (RNN). +
Usually, a more complex types of RNN are used, such as Gated recurrent units (GRU) & Long-Short-Term-Memory (LSTM).

.Encoding “the cat likes to eat pizza”
image::..\images\encoder-exp.png[]


=== Long-Short-Term-Memory

GNMT uses LSTMs for encoding and decoding. +
The problem is, that deep (stacked) LSTMs are hard to train (vanishing\exploding gradient).


=== Long-Short-Term-Memory

.On the left: simple stacked LSTM layer. On the right: GNMT implementation of stacked LSTM layers with residual connections.
image::..\images\res-lstm.png[]

[.notes]
--
With residual connections, input to the bottom LSTM layer ( x~i~^0^ ’s to LSTM~1~) 
is element-wise added to the output from the bottom layer ( x~i~^1^ ’s). This sum is then fed to the top LSTM layer (LSTM~2~) as the new input.
--


=== Attention

The following visualization shows the progression of GNMT as it translates a Chinese sentence to English

image::..\images\gnmt.gif[]

[.notes]
--

* Encodes Chinese words as list of vectors.
* Each vector represent the meaning of all words so far.

* After Encoder finish, Decoder translate one word at a time
 
* Attention: weighted distribution over encoded Chinese vectors most relevant to generate the English word.

--

== Model Architecture

image::..\images\entire-model.png[]

[.notes]
--
* 8 Encoder LSTM Layers
** Pink left->right, Green right->left
** Residual connections from 3rd layer
** First two layers are considers as bi-directional LSTM.


The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green
nodes gather information from right to left. 

The other layers of the encoder are uni-directional. 

Residual connections start from the layer third from the bottom in the encoder and decoder. 

The model is partitioned into multiple GPUs to speed up training. 

We have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers. 
With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine. 

During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers
can start computing, each on a separate GPU. 

To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers. 

The softmax layer is also partitioned and placed on multiple GPUs. 

Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.
--

== Translation

[width="100%",h, cols="1,5"]
|====================
| Input     | Uno no es lo que es por lo que escribe, sino por lo que ha leído.
| SMT       | One is not what is for what he writes, but for what he has read. 
| GNMT      | You are not what you write, but what you have read. 
|====================

[.notes]
--
Human: +
You are who you are not because of what you have written, but because of what you have read.
--

== Overall Results

image::..\images\translate-models.png[]

[.notes]
--
* results show that our model reduces translation errors by more than 60% compared to the PBMT model on
these major pairs of languages

* human perception of the translation quality. We asked
human raters to rate translations in a three-way side-by-side comparison. 

The three sides are from: 
1) translations from the production phrase-based statistical translation system used by Google, 
2) translations from our GNMT system 
3) translations by humans fluent in both languages.

--


== Additional notes

Google used this beast to provide **Zero-Shot translation**.

image::..\images\zero-shot.gif[]

[.notes]
--
* No change in GNMT, additional "token" to specify target language

* Improving translation & “Zero-Shot Translation” — translation between language pairs never seen explicitly by the system.

Here’s how it works.

* Train on Japanese⇄English and Korean⇄English examples ([blue]#Solid blue#)

* The system shares its trained parameters between four different language systems.

* This sharing enables the system to transfer the “translation knowledge” from one language pair to the others. +
This transfer learning and the need to translate between multiple languages forces the system to better use its modeling power.

* Translate between language pair never seen before (between Korean and Japanese)

* It can generate reasonable Korean⇄Japanese translations

* Zero-Shot translation ([yellow]#yellow dotted lines#).
 
To the best of our knowledge, this is the first time this type of transfer learning has worked in Machine Translation.

--

== Questions?

image::..\images\que.gif[,70%]